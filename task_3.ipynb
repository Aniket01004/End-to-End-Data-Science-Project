{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060887b5-9fac-4d97-bd4c-9eaa03aa2aaf",
   "metadata": {},
   "source": [
    "Step 1:- Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d32a1f-aa2c-461c-8a94-47ff17dc8c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aniket\n",
      "[nltk_data]     Bhoge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7c73a-a431-41d1-a208-aad81adb20bb",
   "metadata": {},
   "source": [
    "Step 2:- Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ea18d63-a096-4287-88e7-6e3c4c45016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad4c09-02d9-4533-90dd-ae3a67e6040a",
   "metadata": {},
   "source": [
    "Step 3:- Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7ef652-df60-4d91-80ff-66a22a2310fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['cleaned'] = df['message'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a77896-6d49-461c-aa7c-11470f197e24",
   "metadata": {},
   "source": [
    "Step 4:- TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e8e6c36-c805-4ac9-8fa0-0d75790c1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94245aea-95ce-4926-9bfe-1f84af2bf000",
   "metadata": {},
   "source": [
    "Step 5:- Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f18554b-6d32-445d-89de-4a9a100fc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32bb85-c0d3-4228-9764-4e00a688b270",
   "metadata": {},
   "source": [
    "Step 6:- Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d741ab63-dada-419c-8e4f-a82b55dc9617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9659192825112107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       965\n",
      "           1       1.00      0.75      0.85       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.87      0.92      1115\n",
      "weighted avg       0.97      0.97      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa778995-5657-4e53-b279-09e97248cba3",
   "metadata": {},
   "source": [
    "Step 7:- Save Model & Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863f7e9f-c1cf-4b16-a792-76ba6c7f5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save model and vectorizer\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ba2aa-aa9a-437a-915d-a29d52c9dfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
